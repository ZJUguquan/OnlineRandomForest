% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Tree.R
\docType{data}
\name{ORF}
\alias{ORF}
\title{Create a Online Random Forest Object}
\format{\code{\link{R6Class}} object.}
\usage{
ORF$new(param, numTrees = 100)
}
\arguments{
\item{param}{A list which usually has names of \code{minSamples, minGain, numClasses, x.rng, etc.}.
More details show in \code{\link[=ORT]{Online Random Tree}}.}

\item{numTrees}{A nonnegative integer indicates how many ORT trees are going to build.}
}
\value{
Object of \code{\link{R6Class}}, Object of \code{Online Random Forest}.
}
\description{
ORF is a class of R6.
You can use it to create a \strong{random forest} via diffrent ways, which supports incremental learning as well as batch learning.
As a matter of fact, the Online Random Forest is made of a list of \code{\link[=ORT]{Online Random Tree}}.
}
\details{
Online Random Forest was first introduced by \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.1671&rep=rep1&type=pdf}{Amir Saffari}.
After that, \href{https://github.com/luiarthur/ORFpy}{Arthur Lui} has implemented the algorithm using Python.
Follwed by the paper and Lui's implemention, I refactor the code via R and R6 package. In additon,
my implemention of ORF support both incremental learning and batch learning by combined with \code{randomForest}.
For usage, see details in description of each field or method.
}
\section{Fields}{

\describe{
\item{\code{age}}{How many times has the loop go through inside the \code{update()} function.}
\item{\code{minSamples}}{A part of \code{param} indicates the minimal samples in a leaf node}
\item{\code{minGain}}{A part of \code{param} indicates minimal entropy gain when split a node.}
\item{\code{numTests}}{A part of \code{param} indicates the number of \code{SuffStats} in tests. Default 10 if not set.}
\item{\code{maxDepth}}{A part of \code{param} indicates max depth of an ORT tree. Default 10 if not set.}
\item{\code{numClasses}}{A nonnegative integer indicates how many classes when solve a classifation problem. Default 0 for regression. If numClasses > 0, then do classifation.}
\item{\code{classValues}}{All diffrent possible values of y if classification. Default NULL if not set.}
\item{\code{x.rng}}{
A data frame which indicates the range of every x variable in training data.
It must be a shape of \code{n*2} which n is the number of x variables, i.e. \code{x.dim}.
And the first collumn must be the minimal values of x and the second as maximum.
You can generate it via \code{OnlineRandomForest::dataRange()} for convenience.
}
\item{\code{...}}{Other fields can be seen in \code{\link{Tree}}.}
}
}

\section{Methods}{

\describe{
\item{\code{findLeaf(x, tree, depth = 0)}}{
Find the leaf node where x is located. Return a list of node and its depth.
\itemize{
\item x - A sample of x. \cr
\item tree - An ORT tree or node.
}
}
\item{\code{gains(elem)}}{
Compute the entropy gain on all tests of the elem.
\itemize{
\item elem - An \code{Elem} object.
}
}
\item{\code{update(x, y)}}{
When a sample comes in current node, update ORT with the sample's x variables and y value. \cr
\itemize{
\item x - The x variables of a sample. Note it is an numeric vector other than a scalar.
\item y - The y value of a sample.
}
}
\item{\code{generateTree(tree.mat, df.node, node.ind = 1)}}{
Generate a Tree from a tree matrix which just likes the result of \code{randomForest::getTree()}.\cr
\itemize{
\item tree.mat - A tree matrix which can be obtained from \code{randomForest::getTree()}. Node that it must have a column named \strong{node.ind}. See \strong{Examples}. \cr
\item node.ind - The index of the current node in Tree. Default \code{1} for the root node. For most purposes, don't need to change it.
\item df.node - The training data frame used when constructing random forest,
i.e., the \strong{data} argument in \code{\link[randomForest]{randomForest::randomForest()}} function.
}
}
\item{\code{predict(x)}}{
Predict the corresponding y value of x.
\itemize{
\item x - The x variables of a sample. Note it is an numeric vector other than a scalar.
}
}
\item{\code{draw()}}{Draw the Tree.}
\item{\code{...}}{Other methods can be seen in \code{\link{Tree}}.}
}
}

\examples{
# regression example
if(!require(ggplot2)) install.packages("ggplot2")
data("diamonds", package = "ggplot2")
dat <- as.data.frame(diamonds[sample(1:53000,1000), c(1:6,8:10,7)])
for (col in c("cut","color","clarity")) dat[[col]] <- as.integer(dat[[col]])
x.rng <- data.frame(min = apply(dat[1:9], 2, min),
                     max = apply(dat[1:9], 2, max),
                     row.names = paste0("X", 1:9))
param <- list('minSamples'= 10, 'minGain'= 1, 'maxDepth' = 10, 'x.rng'= x.rng)
ind.gen <- sample(1:1000,500)
ind.updt <- setdiff(1:1000, ind.gen)
rf <- randomForest(price ~ ., data = dat[ind.gen2,], maxnodes = 20, ntree = 100)
orf <- ORF$new(param2)
orf$generateForest(rf, df.train = dat[ind.gen2,], y.col = "price") # 9s,114MB
for (i in sample(ind.updt2,100)) {
  orf$update(dat[i,1:9], dat[i,10])
} # 35s

preds <- orf$predicts(dat[901:1000, 1:9])
Metrics::rmse(preds, dat$price[901:1000])

preds.rf <- predict(rf, newdata = dat[901:1000,])
Metrics::rmse(preds.rf, dat$price[901:1000])
}
\author{
Quan Gu
}
\keyword{datasets}
