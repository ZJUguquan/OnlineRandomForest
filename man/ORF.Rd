% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ORForest.R
\docType{data}
\name{ORF}
\alias{ORF}
\title{Create a Online Random Forest Object}
\format{\code{\link{R6Class}} object.}
\usage{
ORF$new(param, numTrees = 100)
}
\arguments{
\item{param}{A list which usually has names of \code{minSamples, minGain, numClasses, x.rng, etc.}.
More details show in \code{\link[=ORT]{Online Random Tree}}.}

\item{numTrees}{A nonnegative integer indicates how many ORT trees are going to build.}
}
\value{
Object of \code{\link{R6Class}}, Object of \code{Online Random Forest}.
}
\description{
ORF is a class of R6.
You can use it to create a \strong{random forest} via diffrent ways, which supports incremental learning as well as batch learning.
As a matter of fact, the Online Random Forest is made of a list of \code{\link[=ORT]{Online Random Trees}}.
}
\details{
Online Random Forest was first introduced by \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.1671&rep=rep1&type=pdf}{Amir Saffari, etc}.
After that, \href{https://github.com/luiarthur/ORFpy}{Arthur Lui} has implemented the algorithm using Python.
Following the paper's advice and Lui's implemention, I refactor the code via R and R6 package. In additon,
the implemention of ORF in this package support both incremental learning and batch learning by combining with \code{\link[randomForest]{randomForest}}.
For usage, see details in description of each field or method.
}
\section{Fields}{

\describe{
\item{\code{numClasses}}{A nonnegative integer indicates how many classes when solve a classifation problem. Default 0 for regression. If numClasses > 0, then do classifation.}
\item{\code{classify}}{TRUE for classification and FALSE for Regression, depending on the value of \code{numClasses}.}
\item{\code{forest}}{A list of ORT trees. More details show in \code{\link[=ORT]{Online Random Tree}}.}
}
}

\section{Methods}{

\describe{
\item{\code{update(x, y)}}{
When a sample comes in, update all ORT trees in forest with the sample's x variables and y value. \cr
\itemize{
\item x - The x variables of a sample. Note it is an numeric vector other than a scalar.
\item y - The y value of a sample.
}
}
\item{\code{generateForest(rf, df.train, y.col)}}{
Generate a list of ORT trees, call function \code{\link[=ORT]{ORT$generateTree()}} inside.\cr
\itemize{
\item tree.mat - A tree matrix which can be obtained from \code{randomForest::getTree()}. Node that it must have a column named \strong{node.ind}. See \strong{Examples}. \cr
\item df.train -  The training data frame which has been used to contruct randomForest,
i.e., the \strong{data} argument in \code{\link[randomForest]{randomForest}} function.
}
}
\item{\code{predict(x)}}{
Predict the corresponding y value of x, using all ORT trees.
\itemize{
\item x - The x variables of a sample. Note it is an numeric vector other than a scalar.
}
}
\item{\code{predicts(X)}}{
Predict the corresponding y value of x, using all ORT trees.
\itemize{
\item X - A matrix or a data frame corresponding to a batch of samples' x variables.
}
}
\item{\code{confusionMatrix(X, y, pretty = FALSE)}}{
Get a confusion matrix about predicted y values and true y values. Only for classification problem.
\itemize{
\item X - A matrix or a data frame corresponding to a batch of samples' x variables.
\item y - A vector of y values corresponding to a batch of samples.
\item pretty - If TRUE, print a pretty confusion matrix (need \code{gmodels} package). Default FALSE.
}
}
\item{\code{meanTreeSize()}}{Mean size of ORT trees in the forest.}
\item{\code{meanNumLeaves()}}{Mean leaf nodes numbers of ORT trees in the forest.}
\item{\code{meanTreeDepth()}}{Mean depth of ORT trees in the forest.}
\item{\code{sdTreeSize()}}{Standard deviation for size of ORT trees in the forest.}
\item{\code{sdTreeSize()}}{Standard deviation for leaf nodes numbers of ORT trees in the forest.}
\item{\code{sdTreeSize()}}{Standard deviation for depth of ORT trees in the forest.}
}
}

\examples{
# regression example
if(!require(ggplot2)) install.packages("ggplot2")
data("diamonds", package = "ggplot2")
dat <- as.data.frame(diamonds[sample(1:53000,1000), c(1:6,8:10,7)])
for (col in c("cut","color","clarity")) dat[[col]] <- as.integer(dat[[col]])
x.rng <- data.frame(min = apply(dat[1:9], 2, min),
                    max = apply(dat[1:9], 2, max),
                    row.names = paste0("X", 1:9))
param <- list('minSamples'= 10, 'minGain'= 1, 'maxDepth' = 10, 'x.rng'= x.rng)
ind.gen <- sample(1:1000, 800)
ind.updt <- sample(setdiff(1:1000, ind.gen), 100)
ind.test <- setdiff(setdiff(1:1000, ind.gen), ind.updt)
rf <- randomForest(price ~ ., data = dat[ind.gen, ], maxnodes = 20, ntree = 100)
orf <- ORF$new(param)
orf$generateForest(rf, df.train = dat[ind.gen, ], y.col = "price")
orf$meanTreeSize()
for (i in ind.updt) {
  orf$update(dat[i, 1:9], dat[i, 10])
}
orf$meanTreeSize()

if(!require(Metrics)) install.packages("Metrics")
preds <- orf$predicts(dat[ind.test, 1:9])
Metrics::rmse(preds, dat$price[ind.test])
preds.rf <- predict(rf, newdata = dat[ind.test,])
Metrics::rmse(preds.rf, dat$price[ind.test]) # make progress

# classification example
# Just help yourself, boy !

}
\author{
Quan Gu
}
\keyword{datasets}
